INFO:     Started server process [84876]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO:     127.0.0.1:46782 - "GET /health HTTP/1.1" 200 OK
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
WARNING:transformers_modules.jinaai.xlm_hyphen_roberta_hyphen_flash_hyphen_implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.
WARNING:transformers_modules.jinaai.xlm_hyphen_roberta_hyphen_flash_hyphen_implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.
WARNING:transformers_modules.jinaai.xlm_hyphen_roberta_hyphen_flash_hyphen_implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.
WARNING:transformers_modules.jinaai.xlm_hyphen_roberta_hyphen_flash_hyphen_implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.
WARNING:transformers_modules.jinaai.xlm_hyphen_roberta_hyphen_flash_hyphen_implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.
WARNING:transformers_modules.jinaai.xlm_hyphen_roberta_hyphen_flash_hyphen_implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.
WARNING:transformers_modules.jinaai.xlm_hyphen_roberta_hyphen_flash_hyphen_implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.
WARNING:transformers_modules.jinaai.xlm_hyphen_roberta_hyphen_flash_hyphen_implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.
WARNING:transformers_modules.jinaai.xlm_hyphen_roberta_hyphen_flash_hyphen_implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.
WARNING:transformers_modules.jinaai.xlm_hyphen_roberta_hyphen_flash_hyphen_implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.
WARNING:transformers_modules.jinaai.xlm_hyphen_roberta_hyphen_flash_hyphen_implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.
WARNING:transformers_modules.jinaai.xlm_hyphen_roberta_hyphen_flash_hyphen_implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.
WARNING:transformers_modules.jinaai.xlm_hyphen_roberta_hyphen_flash_hyphen_implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.
WARNING:transformers_modules.jinaai.xlm_hyphen_roberta_hyphen_flash_hyphen_implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.
WARNING:transformers_modules.jinaai.xlm_hyphen_roberta_hyphen_flash_hyphen_implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.
WARNING:transformers_modules.jinaai.xlm_hyphen_roberta_hyphen_flash_hyphen_implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.
WARNING:transformers_modules.jinaai.xlm_hyphen_roberta_hyphen_flash_hyphen_implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.
WARNING:transformers_modules.jinaai.xlm_hyphen_roberta_hyphen_flash_hyphen_implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.
WARNING:transformers_modules.jinaai.xlm_hyphen_roberta_hyphen_flash_hyphen_implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.
WARNING:transformers_modules.jinaai.xlm_hyphen_roberta_hyphen_flash_hyphen_implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.
WARNING:transformers_modules.jinaai.xlm_hyphen_roberta_hyphen_flash_hyphen_implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.
WARNING:transformers_modules.jinaai.xlm_hyphen_roberta_hyphen_flash_hyphen_implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.
WARNING:transformers_modules.jinaai.xlm_hyphen_roberta_hyphen_flash_hyphen_implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.
WARNING:transformers_modules.jinaai.xlm_hyphen_roberta_hyphen_flash_hyphen_implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.
WARNING:transformers_modules.jinaai.xlm_hyphen_roberta_hyphen_flash_hyphen_implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Encoding:   0%|          | 0/1 [00:00<?, ?it/s]Encoding: 100%|██████████| 1/1 [00:00<00:00,  2.78it/s]Encoding: 100%|██████████| 1/1 [00:00<00:00,  2.77it/s]
INFO:     127.0.0.1:46792 - "POST /api/embeddings HTTP/1.1" 200 OK
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:16,  5.65s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:12<00:12,  6.20s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:18<00:06,  6.32s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  4.45s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.07s/it]
Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.
INFO:     127.0.0.1:59184 - "POST /api/generate HTTP/1.1" 200 OK
Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.
INFO:     127.0.0.1:48466 - "POST /api/generate HTTP/1.1" 200 OK
Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.
INFO:     127.0.0.1:43556 - "POST /api/generate HTTP/1.1" 200 OK
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:04,  2.21s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:05<00:02,  2.69s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.77s/it]
INFO:     127.0.0.1:43112 - "POST /api/generate HTTP/1.1" 500 Internal Server Error
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/uvicorn/protocols/http/httptools_impl.py", line 416, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/fastapi/applications.py", line 1135, in __call__
    await super().__call__(scope, receive, send)
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/fastapi/routing.py", line 115, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/fastapi/routing.py", line 101, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/fastapi/routing.py", line 355, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/fastapi/routing.py", line 243, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/gpu-scripts/torch_fastapi.py", line 292, in generate_endpoint
    outputs = llm_model.generate(
              ^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/transformers/generation/utils.py", line 2543, in generate
    prepared_logits_processor = self._get_logits_processor(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/transformers/generation/utils.py", line 1267, in _get_logits_processor
    processors.append(TemperatureLogitsWarper(generation_config.temperature))
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/transformers/generation/logits_process.py", line 287, in __init__
    raise ValueError(except_msg)
ValueError: `temperature` (=0.0) has to be a strictly positive float, otherwise your next token scores will be invalid. If you're looking for greedy decoding strategies, set `do_sample=False`.
INFO:     127.0.0.1:57230 - "POST /api/generate HTTP/1.1" 500 Internal Server Error
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/uvicorn/protocols/http/httptools_impl.py", line 416, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/fastapi/applications.py", line 1135, in __call__
    await super().__call__(scope, receive, send)
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/fastapi/routing.py", line 115, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/fastapi/routing.py", line 101, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/fastapi/routing.py", line 355, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/fastapi/routing.py", line 243, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/gpu-scripts/torch_fastapi.py", line 292, in generate_endpoint
    outputs = llm_model.generate(
              ^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/transformers/generation/utils.py", line 2543, in generate
    prepared_logits_processor = self._get_logits_processor(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/transformers/generation/utils.py", line 1267, in _get_logits_processor
    processors.append(TemperatureLogitsWarper(generation_config.temperature))
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/pypoetry/virtualenvs/gpu-scripts-0KjOeIt0-py3.11/lib/python3.11/site-packages/transformers/generation/logits_process.py", line 287, in __init__
    raise ValueError(except_msg)
ValueError: `temperature` (=0.0) has to be a strictly positive float, otherwise your next token scores will be invalid. If you're looking for greedy decoding strategies, set `do_sample=False`.
INFO:     127.0.0.1:60972 - "POST /api/generate HTTP/1.1" 200 OK
Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]Fetching 5 files:  20%|██        | 1/5 [02:20<09:22, 140.59s/it]Fetching 5 files: 100%|██████████| 5/5 [02:20<00:00, 28.12s/it] 
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:01<00:06,  1.59s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:04<00:06,  2.19s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:06<00:04,  2.38s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:08<00:02,  2.27s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:09<00:00,  1.51s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:09<00:00,  1.82s/it]
INFO:     127.0.0.1:52718 - "POST /api/generate HTTP/1.1" 200 OK
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO:     Finished server process [84876]
