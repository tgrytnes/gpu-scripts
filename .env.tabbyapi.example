# =============================================================================
# TabbyAPI Configuration
# =============================================================================
# Copy relevant sections to .env and customize for your setup

# =============================================================================
# Model Configuration
# =============================================================================

# HuggingFace model repository (EXL2 quantized models)
TABBY_MODEL_REPO=bartowski/Qwen2.5-Coder-32B-Instruct-exl2
TABBY_MODEL_REVISION=4.0bpw

# Alternative models (uncomment to use):
# TABBY_MODEL_REPO=bartowski/Qwen2.5-Coder-32B-Instruct-exl2
# TABBY_MODEL_REVISION=6.0bpw  # Higher quality, needs more VRAM

# TABBY_MODEL_REPO=turboderp/Llama-3.1-70B-Instruct-exl2
# TABBY_MODEL_REVISION=4.0bpw  # Requires 48GB+ VRAM

# TABBY_MODEL_REPO=bartowski/deepseek-coder-33b-instruct-exl2
# TABBY_MODEL_REVISION=4.0bpw

# =============================================================================
# Model Directory
# =============================================================================

# Where to store downloaded models
# RunPod: /workspace/models (persistent across restarts)
# Other: /root/models or custom path
TABBY_MODEL_DIR=/workspace/models

# =============================================================================
# Server Configuration
# =============================================================================

# API server port
TABBY_PORT=5000

# Host (0.0.0.0 for external access, 127.0.0.1 for local only)
TABBY_HOST=0.0.0.0

# Maximum sequence length (context window)
# For 32B 4.0bpw on 24GB: 8192 is safe, 16384 may work
TABBY_MAX_SEQ_LEN=8192

# =============================================================================
# Authentication (Optional)
# =============================================================================

# API key for authentication (leave empty to disable)
# TABBY_API_KEY=your-secret-key-here

# =============================================================================
# HuggingFace Configuration
# =============================================================================

# HuggingFace token for downloading gated models
# Get yours at: https://huggingface.co/settings/tokens
# HF_TOKEN=hf_xxxxxxxxxxxxx

# =============================================================================
# EXL2 Quantization Guide
# =============================================================================
# EXL2 bits-per-weight (bpw) guide for RTX 3090/4090 (24GB VRAM):
#
# 32B Models:
#   - 3.0bpw  (~16GB) - Lower quality, more context possible
#   - 4.0bpw  (~20GB) - Recommended balance ‚≠ê
#   - 5.0bpw  (~24GB) - Higher quality, tight fit
#   - 6.0bpw  (~28GB) - Requires 32GB+ VRAM
#
# 70B Models:
#   - 2.5bpw  (~22GB) - Fits on 24GB, lower quality
#   - 3.0bpw  (~26GB) - Needs 32GB VRAM
#   - 4.0bpw  (~35GB) - Needs 48GB VRAM
#
# Note: Lower bpw = less VRAM but slightly lower quality
#       Higher bpw = more VRAM but better quality
#
# For 24GB GPUs, 4.0bpw is the sweet spot for 32B models
# =============================================================================

# =============================================================================
# Popular EXL2 Model Repositories
# =============================================================================
#
# Coding Models:
#   - bartowski/Qwen2.5-Coder-32B-Instruct-exl2 (Best for code)
#   - bartowski/deepseek-coder-33b-instruct-exl2
#   - turboderp/CodeLlama-34b-Instruct-hf-exl2
#
# General Purpose:
#   - turboderp/Llama-3.1-70B-Instruct-exl2
#   - bartowski/Qwen2.5-32B-Instruct-exl2
#   - turboderp/Mistral-Large-Instruct-2407-exl2
#
# Find more at: https://huggingface.co/bartowski or https://huggingface.co/turboderp
# =============================================================================
