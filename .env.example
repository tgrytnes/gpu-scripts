# =============================================================================
# vLLM Configuration
# =============================================================================
# Copy this file to .env and customize for your setup

# Model Configuration
# Choose from HuggingFace models (examples below)
VLLM_MODEL_ID=meta-llama/Llama-3.2-3B-Instruct
# VLLM_MODEL_ID=meta-llama/Llama-3.1-8B-Instruct
# VLLM_MODEL_ID=mistralai/Mistral-7B-Instruct-v0.3
# VLLM_MODEL_ID=Qwen/Qwen2.5-7B-Instruct
# VLLM_MODEL_ID=google/gemma-2-9b-it

# Server Configuration
VLLM_PORT=8000
VLLM_HOST=0.0.0.0

# GPU Configuration (for RTX 3090/4090 24GB)
VLLM_GPU_MEMORY_UTILIZATION=0.90  # Use 90% of GPU memory
VLLM_MAX_MODEL_LEN=8192           # Max sequence length
VLLM_DTYPE=auto                   # auto, float16, bfloat16

# Optional: API Key for authentication
# VLLM_API_KEY=your-secret-key-here

# Optional: HuggingFace token for gated models
# HF_TOKEN=hf_xxxxxxxxxxxxx

# Project Directory (default: $HOME/gpu-scripts)
# PROJECT_DIR=/workspace/gpu-scripts

# =============================================================================
# Model Size Guide for RTX 3090/4090 (24GB VRAM)
# =============================================================================
# Small (fits easily):
#   - Llama-3.2-3B       (~6GB)
#   - Qwen2.5-3B         (~6GB)
#
# Medium (recommended):
#   - Llama-3.1-8B       (~16GB)
#   - Mistral-7B         (~14GB)
#   - Qwen2.5-7B         (~14GB)
#
# Large (tight fit):
#   - Llama-3.1-8B-16bit (~16GB with max context)
#   - Gemma-2-9B         (~18GB)
#
# Note: Use quantized models (GPTQ/AWQ) for larger models
# =============================================================================
